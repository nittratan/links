import pandas as pd
import json
from transformers import pipeline
from tqdm import tqdm

# ===================== CONFIG =====================
file_path   = "results.csv"   # same file for input/output
source_col  = "context"       # input column
model_name  = "meta-llama/Meta-Llama-3-8B-Instruct"
chunk_chars = 1800            # size of each chunk (characters)
overlap     = 200             # overlap between chunks to avoid cutting sentences
max_new_tok = 700             # generation cap per chunk
# ==================================================

# ---- Load CSV ----
df = pd.read_csv(file_path)

# ---- Load model pipeline ----
# (No explicit device passed to pipeline; device_map handled internally)
generator = pipeline(
    "text-generation",
    model=model_name,
    torch_dtype="auto",
    device_map="auto",
    do_sample=False
)

# ---- JSON skeleton & helpers ----
SKELETON_KEYS = [
    "Visit Reason",
    "Diagnosis",
    "Past Medical History",
    "Clinical Details",
    "Vital Signs",
    "Exam Results",
    "Lab Results",
    "Imaging Test Results",
    "Hospital and ED course",
    "Discharge Plan",
    "Full HPI",
    "Full Assessment and Plan"
]

def empty_skeleton():
    return {k: [] for k in SKELETON_KEYS}

def ensure_schema(js):
    """Ensure all required keys exist and values are lists of strings."""
    if not isinstance(js, dict):
        return empty_skeleton()
    fixed = empty_skeleton()
    for k in SKELETON_KEYS:
        v = js.get(k, [])
        if isinstance(v, list):
            # coerce non-strings to strings safely
            fixed[k] = [str(x).strip() for x in v if str(x).strip()]
        elif isinstance(v, str) and v.strip():
            fixed[k] = [v.strip()]
        else:
            fixed[k] = []
    return fixed

def merge_jsons(json_list):
    """Merge a list of schema-conformant dicts, de-duplicating while preserving order."""
    merged = empty_skeleton()
    for js in json_list:
        js = ensure_schema(js)
        for key in SKELETON_KEYS:
            for item in js[key]:
                # dedupe with order preservation
                if item not in merged[key]:
                    merged[key].append(item)
    return merged

def find_and_parse_json(text):
    """Extract the outermost JSON object from text and parse it."""
    try:
        start = text.find("{")
        end   = text.rfind("}")
        if start != -1 and end != -1 and end > start:
            return json.loads(text[start:end+1])
    except Exception:
        pass
    return None

def chunk_text(s, size=chunk_chars, ov=overlap):
    if not isinstance(s, str): 
        return []
    s = s.strip()
    if len(s) <= size:
        return [s] if s else []
    chunks = []
    i = 0
    while i < len(s):
        chunk = s[i:i+size]
        chunks.append(chunk)
        i += max(size - ov, 1)
    return chunks

# ---- Prompt builder (keeps explanatory text INSIDE the JSON strings) ----
def build_prompt(text):
    return (
        "Extract the following sections from the medical note and return them as a single JSON object. "
        "Each field must be an array of strings. Each string may include a concise, human-readable explanation "
        "with the extracted detail (e.g., include brief context like 'Impressions:' or short qualifiers). "
        "If a section is not present, return an empty array for that section.\n\n"
        "Required JSON shape:\n"
        '{ "Visit Reason": [], '
        '"Diagnosis": [], '
        '"Past Medical History": [], '
        '"Clinical Details": [], '
        '"Vital Signs": [], '
        '"Exam Results": [], '
        '"Lab Results": [], '
        '"Imaging Test Results": [], '
        '"Hospital and ED course": [], '
        '"Discharge Plan": [], '
        '"Full HPI": [], '
        '"Full Assessment and Plan": [] }\n\n'
        "Medical note:\n"
        f"{text}\n\n"
        "Return only the JSON object now:"
    )

def extract_from_chunk(text):
    prompt = build_prompt(text)
    out = generator(prompt, max_new_tokens=max_new_tok)[0]["generated_text"]
    parsed = find_and_parse_json(out)
    return ensure_schema(parsed) if parsed else empty_skeleton()

def extract_info_large(text):
    if not isinstance(text, str) or not text.strip():
        return json.dumps(empty_skeleton(), ensure_ascii=False)

    # Split into overlapping chunks, extract each, then merge
    chunks = chunk_text(text, size=chunk_chars, ov=overlap)
    if not chunks:
        return json.dumps(empty_skeleton(), ensure_ascii=False)

    partials = []
    for ch in chunks:
        js = extract_from_chunk(ch)
        partials.append(js)

    merged = merge_jsons(partials)
    return json.dumps(merged, ensure_ascii=False)

# ---- Apply with progress bar and save back to SAME file ----
tqdm.pandas(desc="Extracting JSON with explanations (chunked)")
df[model_name] = df[source_col].progress_apply(extract_info_large)

df.to_csv(file_path, index=False)
print(f"âœ… Processing complete. Updated file saved: {file_path}")
